#
# Ingest ft_transcendence-like JSON logs and send to Elasticsearch over HTTPS.
# - Reads ONLY JSON lines; any non-JSON lines (debug/plain text) are dropped.
# - Converts epoch-millis "time" to @timestamp.
# - Coerces types for easier aggregations in Kibana.
# logstash/pipeline/logstash.conf


input {
  file {
    path => ["/logs/*"]
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb"
    codec => json              # parse JSON at the source
  }
}

filter {
  # If a non-JSON line sneaks in, codec->json will tag it. Drop those.
  if "jsonparsefailure" in [tags] {
    drop { }
  }

  # Map Pino/Fastify time (epoch ms) to @timestamp
  if [time] {
    # convert to string first; then to UNIX_MS
    mutate { convert => { "time" => "string" } }
    date {
      match => [ "time", "UNIX_MS" ]
      target => "@timestamp"
      remove_field => [ "time" ]   # keep the event clean; optional
    }
  }

  # Optional: coerce numeric types when present
  mutate {
    convert => {
      "level" => "integer"
      "pid"   => "integer"
      "[res][statusCode]" => "integer"
      "responseTime" => "float"
    }
  }
}

output {
  elasticsearch {
    hosts => ["https://elasticsearch:9200"]
    user => "${LS_ES_USER}"
    password => "${LS_ES_PASS}"
    ssl_enabled => true
    #ssl => true
    #cacert => "/usr/share/logstash/certs/ca/ca.crt"
    ssl_certificate_authorities => ["/usr/share/logstash/certs/ca/ca.crt"]
    # Use a data stream or index pattern under an ILM policy:
    #index => "ft-logs-%{+yyyy.MM.dd}"
    #rollover alias.
    index => "ftt-logs-json"
  }
  # stdout { codec => rubydebug }  # uncomment for troubleshooting
}
